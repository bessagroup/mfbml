{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f501432c090>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the configuration of the low-fidelity model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from mfbml.methods.dnn_lr_bnn import DNNLinearRegressionBNN as DNNLRBNN\n",
    "\n",
    "# fix the random seed for reproducibility\n",
    "seed = 1999\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_pickle(\"../data_generation/data.pkl\")\n",
    "# extract dataset\n",
    "samples = data[\"samples\"]\n",
    "hf_samples = samples[0]\n",
    "lf_samples = samples[1]\n",
    "hf_responses = data[\"responses_lf1\"][0]\n",
    "responses_lf1 = data[\"responses_lf1\"]\n",
    "responses_lf2 = data[\"responses_lf2\"]\n",
    "responses_lf3 = data[\"responses_lf3\"]\n",
    "test_samples = data[\"test_samples\"]\n",
    "test_hf_responses_noiseless = data[\"test_hf_responses_noiseless\"]\n",
    "test_hf_responses = data[\"test_hf_responses\"]\n",
    "test_lf1_responses = data[\"test_lf1_responses\"]\n",
    "test_lf2_responses = data[\"test_lf2_responses\"]\n",
    "test_lf3_responses = data[\"test_lf3_responses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the configuration of the low-fidelity model\n",
    "lf_configure = {\n",
    "    \"in_features\": 1,\n",
    "    \"hidden_features\": [50, 50],\n",
    "    \"out_features\": 1,\n",
    "    \"activation\": \"Tanh\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.000001,\n",
    "    \"loss\": \"mse\",\n",
    "}\n",
    "# create the configuration of the high-fidelity model\n",
    "hf_parallel_configure = {\n",
    "    \"in_features\": 1,\n",
    "    \"hidden_features\": [512, 512],\n",
    "    \"out_features\": 1,\n",
    "    \"activation\": \"Tanh\",\n",
    "    \"lr\": 0.001,\n",
    "    \"sigma\": 0.05,\n",
    "}\n",
    "\n",
    "# lf train config\n",
    "lf_train_config = {\n",
    "    \"batch_size\": None,\n",
    "    \"num_epochs\": 10000,\n",
    "    \"print_iter\": 100,\n",
    "    \"data_split\": False,\n",
    "}\n",
    "hf_train_config = {\n",
    "    \"num_epochs\": 30000,\n",
    "    \"sample_freq\": 100,\n",
    "    \"print_info\": True,\n",
    "    \"burn_in_epochs\": 20000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data split: use all data for training\n",
      "epoch:  100 train loss:  0.9584435820579529\n",
      "epoch:  200 train loss:  0.9574099183082581\n",
      "epoch:  300 train loss:  0.9541923403739929\n",
      "epoch:  400 train loss:  0.9133488535881042\n",
      "epoch:  500 train loss:  0.8133435845375061\n",
      "epoch:  600 train loss:  0.5631825923919678\n",
      "epoch:  700 train loss:  0.37910962104797363\n",
      "epoch:  800 train loss:  0.0417165532708168\n",
      "epoch:  900 train loss:  0.012330312281847\n",
      "epoch:  1000 train loss:  0.006874925456941128\n",
      "epoch:  1100 train loss:  0.0049567269161343575\n",
      "epoch:  1200 train loss:  0.004295203369110823\n",
      "epoch:  1300 train loss:  0.003997720777988434\n",
      "epoch:  1400 train loss:  0.0038646592292934656\n",
      "epoch:  1500 train loss:  0.003782189218327403\n",
      "epoch:  1600 train loss:  0.003731832606717944\n",
      "epoch:  1700 train loss:  0.00370135810226202\n",
      "epoch:  1800 train loss:  0.003667067503556609\n",
      "epoch:  1900 train loss:  0.003646103898063302\n",
      "epoch:  2000 train loss:  0.003625324694439769\n",
      "epoch:  2100 train loss:  0.003611366730183363\n",
      "epoch:  2200 train loss:  0.0035949491430073977\n",
      "epoch:  2300 train loss:  0.003583483165130019\n",
      "epoch:  2400 train loss:  0.0035704276524484158\n",
      "epoch:  2500 train loss:  0.003560678567737341\n",
      "epoch:  2600 train loss:  0.0035494156181812286\n",
      "epoch:  2700 train loss:  0.0035423897206783295\n",
      "epoch:  2800 train loss:  0.003532927017658949\n",
      "epoch:  2900 train loss:  0.0035220179706811905\n",
      "epoch:  3000 train loss:  0.003514515468850732\n",
      "epoch:  3100 train loss:  0.0035057631321251392\n",
      "epoch:  3200 train loss:  0.003496420569717884\n",
      "epoch:  3300 train loss:  0.003488974180072546\n",
      "epoch:  3400 train loss:  0.0034812872763723135\n",
      "epoch:  3500 train loss:  0.0034741153940558434\n",
      "epoch:  3600 train loss:  0.0034672508481889963\n",
      "epoch:  3700 train loss:  0.003462172346189618\n",
      "epoch:  3800 train loss:  0.0034539795015007257\n",
      "epoch:  3900 train loss:  0.0034477298613637686\n",
      "epoch:  4000 train loss:  0.0034427433274686337\n",
      "epoch:  4100 train loss:  0.003435831982642412\n",
      "epoch:  4200 train loss:  0.0034301450941711664\n",
      "epoch:  4300 train loss:  0.00351064745336771\n",
      "epoch:  4400 train loss:  0.003419233253225684\n",
      "epoch:  4500 train loss:  0.00341418432071805\n",
      "epoch:  4600 train loss:  0.0034088492393493652\n",
      "epoch:  4700 train loss:  0.0034038370940834284\n",
      "epoch:  4800 train loss:  0.0033996417187154293\n",
      "epoch:  4900 train loss:  0.0033941876608878374\n",
      "epoch:  5000 train loss:  0.0034616957418620586\n",
      "epoch:  5100 train loss:  0.0034837177954614162\n",
      "epoch:  5200 train loss:  0.003380857640877366\n",
      "epoch:  5300 train loss:  0.003376968437805772\n",
      "epoch:  5400 train loss:  0.003372796345502138\n",
      "epoch:  5500 train loss:  0.0033703304361552\n",
      "epoch:  5600 train loss:  0.0033656107261776924\n",
      "epoch:  5700 train loss:  0.0033618533052504063\n",
      "epoch:  5800 train loss:  0.003358217654749751\n",
      "epoch:  5900 train loss:  0.0033641632180660963\n",
      "epoch:  6000 train loss:  0.0033514313399791718\n",
      "epoch:  6100 train loss:  0.0033482222352176905\n",
      "epoch:  6200 train loss:  0.003348859492689371\n",
      "epoch:  6300 train loss:  0.0033418862149119377\n",
      "epoch:  6400 train loss:  0.0033781686797738075\n",
      "epoch:  6500 train loss:  0.0033375155180692673\n",
      "epoch:  6600 train loss:  0.0033329660072922707\n",
      "epoch:  6700 train loss:  0.0033294884487986565\n",
      "epoch:  6800 train loss:  0.0033274744637310505\n",
      "epoch:  6900 train loss:  0.003323495388031006\n",
      "epoch:  7000 train loss:  0.0033204497303813696\n",
      "epoch:  7100 train loss:  0.0033196688164025545\n",
      "epoch:  7200 train loss:  0.003314422443509102\n",
      "epoch:  7300 train loss:  0.003311618696898222\n",
      "epoch:  7400 train loss:  0.003308448940515518\n",
      "epoch:  7500 train loss:  0.003306746482849121\n",
      "epoch:  7600 train loss:  0.0033027715981006622\n",
      "epoch:  7700 train loss:  0.003299517324194312\n",
      "epoch:  7800 train loss:  0.0033168261870741844\n",
      "epoch:  7900 train loss:  0.003295961068943143\n",
      "epoch:  8000 train loss:  0.0032913172617554665\n",
      "epoch:  8100 train loss:  0.003291191766038537\n",
      "epoch:  8200 train loss:  0.0032912809401750565\n",
      "epoch:  8300 train loss:  0.0032817870378494263\n",
      "epoch:  8400 train loss:  0.003280359786003828\n",
      "epoch:  8500 train loss:  0.003275772323831916\n",
      "epoch:  8600 train loss:  0.0032868003472685814\n",
      "epoch:  8700 train loss:  0.0032876920886337757\n",
      "epoch:  8800 train loss:  0.0032670770306140184\n",
      "epoch:  8900 train loss:  0.0032642807345837355\n",
      "epoch:  9000 train loss:  0.0032681648153811693\n",
      "epoch:  9100 train loss:  0.003263516118749976\n",
      "epoch:  9200 train loss:  0.0032561933621764183\n",
      "epoch:  9300 train loss:  0.0032534454949200153\n",
      "epoch:  9400 train loss:  0.003252088325098157\n",
      "epoch:  9500 train loss:  0.0032542282715439796\n",
      "epoch:  9600 train loss:  0.003251854097470641\n",
      "epoch:  9700 train loss:  0.0032438405323773623\n",
      "epoch:  9800 train loss:  0.0032485108822584152\n",
      "epoch:  9900 train loss:  0.0032375846058130264\n",
      "epoch:  10000 train loss:  0.003240500809624791\n",
      "==================================================\n",
      "epoch:     0/30000\n",
      "nll_loss: 269.286, prior_loss: 243029.156, total: 243298.438\n",
      "noise: 0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaga/Documents/GitHub/mfbml/src/mfbml/inference/psgld.py:108: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  V.mul_(alpha).addcmul_(1 - alpha, d_p, d_p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "epoch:   100/30000\n",
      "nll_loss: 299.876, prior_loss: 273260.906, total: 273560.781\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   200/30000\n",
      "nll_loss: 276.606, prior_loss: 284524.000, total: 284800.594\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   300/30000\n",
      "nll_loss: 177.264, prior_loss: 294310.219, total: 294487.469\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   400/30000\n",
      "nll_loss: 136.271, prior_loss: 303446.156, total: 303582.438\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   500/30000\n",
      "nll_loss: 108.182, prior_loss: 312017.969, total: 312126.156\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   600/30000\n",
      "nll_loss: 80.448, prior_loss: 319809.812, total: 319890.250\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   700/30000\n",
      "nll_loss: 127.553, prior_loss: 327136.594, total: 327264.156\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   800/30000\n",
      "nll_loss: 89.609, prior_loss: 333935.156, total: 334024.750\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:   900/30000\n",
      "nll_loss: 141.675, prior_loss: 340594.188, total: 340735.875\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1000/30000\n",
      "nll_loss: 31.210, prior_loss: 346502.906, total: 346534.125\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1100/30000\n",
      "nll_loss: 25.199, prior_loss: 352038.469, total: 352063.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1200/30000\n",
      "nll_loss: 11.497, prior_loss: 356996.781, total: 357008.281\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1300/30000\n",
      "nll_loss: 25.122, prior_loss: 361475.219, total: 361500.344\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1400/30000\n",
      "nll_loss: 37.104, prior_loss: 365873.688, total: 365910.781\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1500/30000\n",
      "nll_loss: 4.309, prior_loss: 370212.500, total: 370216.812\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1600/30000\n",
      "nll_loss: 78.886, prior_loss: 374043.469, total: 374122.344\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1700/30000\n",
      "nll_loss: -1.181, prior_loss: 377484.938, total: 377483.750\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1800/30000\n",
      "nll_loss: -1.822, prior_loss: 380800.594, total: 380798.781\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  1900/30000\n",
      "nll_loss: 6.700, prior_loss: 384145.219, total: 384151.906\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2000/30000\n",
      "nll_loss: 78.507, prior_loss: 387162.031, total: 387240.531\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2100/30000\n",
      "nll_loss: -1.798, prior_loss: 389925.781, total: 389923.969\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2200/30000\n",
      "nll_loss: -2.510, prior_loss: 392661.062, total: 392658.562\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2300/30000\n",
      "nll_loss: 23.681, prior_loss: 395200.312, total: 395224.000\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2400/30000\n",
      "nll_loss: 29.391, prior_loss: 397802.344, total: 397831.750\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2500/30000\n",
      "nll_loss: 7.909, prior_loss: 400195.031, total: 400202.938\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2600/30000\n",
      "nll_loss: 2.031, prior_loss: 402586.781, total: 402588.812\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2700/30000\n",
      "nll_loss: 6.927, prior_loss: 404728.781, total: 404735.719\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2800/30000\n",
      "nll_loss: 61.525, prior_loss: 406687.625, total: 406749.156\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  2900/30000\n",
      "nll_loss: 107.345, prior_loss: 408556.000, total: 408663.344\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3000/30000\n",
      "nll_loss: 11.181, prior_loss: 410206.312, total: 410217.500\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3100/30000\n",
      "nll_loss: -0.694, prior_loss: 411871.344, total: 411870.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3200/30000\n",
      "nll_loss: 119.760, prior_loss: 413374.188, total: 413493.938\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3300/30000\n",
      "nll_loss: 5.916, prior_loss: 414954.906, total: 414960.812\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3400/30000\n",
      "nll_loss: -7.375, prior_loss: 416366.656, total: 416359.281\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3500/30000\n",
      "nll_loss: 4.989, prior_loss: 417622.125, total: 417627.125\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3600/30000\n",
      "nll_loss: 6.469, prior_loss: 419030.906, total: 419037.375\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3700/30000\n",
      "nll_loss: 34.887, prior_loss: 420461.750, total: 420496.625\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3800/30000\n",
      "nll_loss: 32.970, prior_loss: 421705.344, total: 421738.312\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  3900/30000\n",
      "nll_loss: 22.956, prior_loss: 422867.875, total: 422890.844\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4000/30000\n",
      "nll_loss: 195.892, prior_loss: 423921.906, total: 424117.812\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4100/30000\n",
      "nll_loss: 27.949, prior_loss: 425018.781, total: 425046.719\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4200/30000\n",
      "nll_loss: 2.947, prior_loss: 425840.250, total: 425843.188\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4300/30000\n",
      "nll_loss: 1.191, prior_loss: 426768.062, total: 426769.250\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4400/30000\n",
      "nll_loss: 37.893, prior_loss: 427705.938, total: 427743.844\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4500/30000\n",
      "nll_loss: 22.707, prior_loss: 428625.969, total: 428648.688\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4600/30000\n",
      "nll_loss: 38.657, prior_loss: 429440.312, total: 429478.969\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4700/30000\n",
      "nll_loss: 6.044, prior_loss: 430306.438, total: 430312.469\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4800/30000\n",
      "nll_loss: 73.469, prior_loss: 431010.844, total: 431084.312\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  4900/30000\n",
      "nll_loss: 30.514, prior_loss: 431828.906, total: 431859.406\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5000/30000\n",
      "nll_loss: 68.286, prior_loss: 432648.375, total: 432716.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5100/30000\n",
      "nll_loss: 7.769, prior_loss: 433249.562, total: 433257.344\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5200/30000\n",
      "nll_loss: 0.006, prior_loss: 433906.938, total: 433906.938\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5300/30000\n",
      "nll_loss: 63.745, prior_loss: 434706.781, total: 434770.531\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5400/30000\n",
      "nll_loss: 4.180, prior_loss: 435342.719, total: 435346.906\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5500/30000\n",
      "nll_loss: 0.317, prior_loss: 435873.750, total: 435874.062\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5600/30000\n",
      "nll_loss: -5.278, prior_loss: 436677.094, total: 436671.812\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5700/30000\n",
      "nll_loss: 6.413, prior_loss: 437348.188, total: 437354.594\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5800/30000\n",
      "nll_loss: -2.321, prior_loss: 438016.250, total: 438013.938\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  5900/30000\n",
      "nll_loss: 126.370, prior_loss: 438618.312, total: 438744.688\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6000/30000\n",
      "nll_loss: 34.876, prior_loss: 438966.875, total: 439001.750\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6100/30000\n",
      "nll_loss: 0.028, prior_loss: 439566.906, total: 439566.938\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6200/30000\n",
      "nll_loss: 38.047, prior_loss: 439893.062, total: 439931.094\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6300/30000\n",
      "nll_loss: 44.147, prior_loss: 440431.219, total: 440475.375\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6400/30000\n",
      "nll_loss: 30.528, prior_loss: 440823.500, total: 440854.031\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6500/30000\n",
      "nll_loss: 67.315, prior_loss: 441333.875, total: 441401.188\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6600/30000\n",
      "nll_loss: 7.168, prior_loss: 441735.938, total: 441743.094\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6700/30000\n",
      "nll_loss: -9.427, prior_loss: 441942.438, total: 441933.000\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6800/30000\n",
      "nll_loss: 14.329, prior_loss: 442412.312, total: 442426.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  6900/30000\n",
      "nll_loss: -4.822, prior_loss: 442928.969, total: 442924.156\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7000/30000\n",
      "nll_loss: 14.523, prior_loss: 443280.219, total: 443294.750\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7100/30000\n",
      "nll_loss: -7.483, prior_loss: 443455.500, total: 443448.031\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7200/30000\n",
      "nll_loss: 3.889, prior_loss: 443819.750, total: 443823.625\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7300/30000\n",
      "nll_loss: -1.607, prior_loss: 444170.281, total: 444168.688\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7400/30000\n",
      "nll_loss: -9.122, prior_loss: 444628.500, total: 444619.375\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7500/30000\n",
      "nll_loss: 24.051, prior_loss: 444869.469, total: 444893.531\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7600/30000\n",
      "nll_loss: 28.753, prior_loss: 445117.594, total: 445146.344\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7700/30000\n",
      "nll_loss: -8.315, prior_loss: 445433.156, total: 445424.844\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7800/30000\n",
      "nll_loss: 19.465, prior_loss: 445851.625, total: 445871.094\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  7900/30000\n",
      "nll_loss: 3.762, prior_loss: 446148.344, total: 446152.094\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8000/30000\n",
      "nll_loss: 1.483, prior_loss: 446483.406, total: 446484.875\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8100/30000\n",
      "nll_loss: 1.500, prior_loss: 446780.031, total: 446781.531\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8200/30000\n",
      "nll_loss: -6.031, prior_loss: 447192.906, total: 447186.875\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8300/30000\n",
      "nll_loss: -5.594, prior_loss: 447490.844, total: 447485.250\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8400/30000\n",
      "nll_loss: 24.397, prior_loss: 447622.438, total: 447646.844\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8500/30000\n",
      "nll_loss: 144.498, prior_loss: 447698.281, total: 447842.781\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8600/30000\n",
      "nll_loss: -7.169, prior_loss: 448052.188, total: 448045.031\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8700/30000\n",
      "nll_loss: 4.154, prior_loss: 448227.719, total: 448231.875\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8800/30000\n",
      "nll_loss: 6.429, prior_loss: 448513.812, total: 448520.250\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  8900/30000\n",
      "nll_loss: 3.020, prior_loss: 448762.094, total: 448765.125\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9000/30000\n",
      "nll_loss: 24.688, prior_loss: 448908.656, total: 448933.344\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9100/30000\n",
      "nll_loss: 11.675, prior_loss: 449047.250, total: 449058.938\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9200/30000\n",
      "nll_loss: 10.950, prior_loss: 449246.750, total: 449257.688\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9300/30000\n",
      "nll_loss: 19.207, prior_loss: 449470.750, total: 449489.969\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9400/30000\n",
      "nll_loss: 9.906, prior_loss: 449696.094, total: 449706.000\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9500/30000\n",
      "nll_loss: 8.337, prior_loss: 449758.594, total: 449766.938\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9600/30000\n",
      "nll_loss: 9.615, prior_loss: 450124.594, total: 450134.219\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9700/30000\n",
      "nll_loss: 72.813, prior_loss: 450385.656, total: 450458.469\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9800/30000\n",
      "nll_loss: 4.074, prior_loss: 450711.594, total: 450715.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch:  9900/30000\n",
      "nll_loss: -1.343, prior_loss: 450829.594, total: 450828.250\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10000/30000\n",
      "nll_loss: 2.416, prior_loss: 451054.719, total: 451057.125\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10100/30000\n",
      "nll_loss: -4.420, prior_loss: 451355.719, total: 451351.312\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10200/30000\n",
      "nll_loss: 17.999, prior_loss: 451594.312, total: 451612.312\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10300/30000\n",
      "nll_loss: 31.880, prior_loss: 451898.781, total: 451930.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10400/30000\n",
      "nll_loss: 10.003, prior_loss: 452017.062, total: 452027.062\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10500/30000\n",
      "nll_loss: 20.979, prior_loss: 452293.062, total: 452314.031\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10600/30000\n",
      "nll_loss: 46.925, prior_loss: 452662.906, total: 452709.844\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10700/30000\n",
      "nll_loss: -3.955, prior_loss: 452945.344, total: 452941.375\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10800/30000\n",
      "nll_loss: 4.472, prior_loss: 453067.688, total: 453072.156\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 10900/30000\n",
      "nll_loss: -5.579, prior_loss: 453202.438, total: 453196.844\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11000/30000\n",
      "nll_loss: 15.266, prior_loss: 453391.906, total: 453407.188\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11100/30000\n",
      "nll_loss: -2.261, prior_loss: 453438.781, total: 453436.531\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11200/30000\n",
      "nll_loss: -3.555, prior_loss: 453540.656, total: 453537.094\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11300/30000\n",
      "nll_loss: -6.271, prior_loss: 453618.750, total: 453612.469\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11400/30000\n",
      "nll_loss: 15.257, prior_loss: 453706.906, total: 453722.156\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11500/30000\n",
      "nll_loss: -5.827, prior_loss: 453789.625, total: 453783.812\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11600/30000\n",
      "nll_loss: 1.281, prior_loss: 454089.906, total: 454091.188\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11700/30000\n",
      "nll_loss: 13.581, prior_loss: 454173.062, total: 454186.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11800/30000\n",
      "nll_loss: 8.464, prior_loss: 454334.594, total: 454343.062\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 11900/30000\n",
      "nll_loss: -7.595, prior_loss: 454472.500, total: 454464.906\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12000/30000\n",
      "nll_loss: 3.890, prior_loss: 454638.312, total: 454642.188\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12100/30000\n",
      "nll_loss: -6.134, prior_loss: 454958.438, total: 454952.312\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12200/30000\n",
      "nll_loss: 19.071, prior_loss: 455014.594, total: 455033.656\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12300/30000\n",
      "nll_loss: 54.470, prior_loss: 454940.031, total: 454994.500\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12400/30000\n",
      "nll_loss: 4.676, prior_loss: 455174.625, total: 455179.312\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12500/30000\n",
      "nll_loss: 17.911, prior_loss: 455318.938, total: 455336.844\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12600/30000\n",
      "nll_loss: -2.014, prior_loss: 455544.125, total: 455542.125\n",
      "noise: 0.133\n",
      "==================================================\n",
      "epoch: 12700/30000\n",
      "nll_loss: -6.942, prior_loss: 455657.906, total: 455650.969\n",
      "noise: 0.133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m DNNLRBNN(\n\u001b[1;32m      5\u001b[0m     design_space\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]]),\n\u001b[1;32m      6\u001b[0m     lf_configure\u001b[38;5;241m=\u001b[39mlf_configure,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     discrepancy_normalization\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# define beta\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlf_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlf_train_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlf_train_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_train_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_train_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# save the model to the disk\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdnn_lr_bnn_lf\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_order_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_512_512_diff.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Documents/GitHub/mfbml/src/mfbml/methods/dnn_lr_bnn.py:248\u001b[0m, in \u001b[0;36mDNNLinearRegressionBNN.train\u001b[0;34m(self, X, Y, lf_train_config, hf_train_config)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_model\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_model\u001b[38;5;241m.\u001b[39msigma \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiff_std\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# train the high-fidelity model (normalized discrepancy)\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_hf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_samples_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdis_hf_lf_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_train_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_train_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_freq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_train_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprint_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mburn_in_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_train_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mburn_in_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/mfbml/src/mfbml/methods/dnn_lr_bnn.py:410\u001b[0m, in \u001b[0;36mDNNLinearRegressionBNN.train_hf_model\u001b[0;34m(self, X, Y, num_epochs, sample_freq, verbose, burn_in_epochs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_hf_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    386\u001b[0m                    X: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    387\u001b[0m                    Y: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m                    burn_in_epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    392\u001b[0m                    ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"train the high-fidelity model\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m        burn in epochs, by default 1000\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mburn_in_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mburn_in_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/mfbml/src/mfbml/methods/bayes_neural_nets.py:279\u001b[0m, in \u001b[0;36mBNNWrapper.train\u001b[0;34m(self, X, Y, num_epochs, sample_freq, burn_in_epochs, verbose, verbose_interval)\u001b[0m\n\u001b[1;32m    277\u001b[0m loss \u001b[38;5;241m=\u001b[39m nll_loss \u001b[38;5;241m+\u001b[39m prior_loss\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# back propagation\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    281\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/mfpml_env/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mfpml_env/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the MFDNNBNN object\n",
    "lf_responses = [responses_lf1, responses_lf2, responses_lf3]\n",
    "for i, lf_response  in enumerate(lf_responses):\n",
    "    model = DNNLRBNN(\n",
    "        design_space=torch.Tensor([[0, 1]]),\n",
    "        lf_configure=lf_configure,\n",
    "        hf_configure=hf_parallel_configure,\n",
    "        beta_optimize=True,\n",
    "        lf_order=1,\n",
    "        beta_bounds=[-5, 5],\n",
    "        discrepancy_normalization=\"diff\",\n",
    "    )\n",
    "    # define beta\n",
    "    model.train(\n",
    "        X=samples,\n",
    "        Y=lf_response,\n",
    "        lf_train_config=lf_train_config,\n",
    "        hf_train_config=hf_train_config,\n",
    "    )\n",
    "    # save the model to the disk\n",
    "    with open(f\"dnn_lr_bnn_lf{i+1}_order_{i+1}_512_512_diff.pkl\", \"wb\") as f:\n",
    "        torch.save(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model (one model for example)\n",
    "with open(\"dnn_lr_bnn_lf1_order_1_512_512_diff.pkl\", \"rb\") as f:\n",
    "    model = torch.load(f)\n",
    "# visualize the posterior of mf-dnn-bnn\n",
    "(\n",
    "    mfdnnbnn_lf1_hy,\n",
    "    mfdnnbnn_lf1_epistemic,\n",
    "    mfdnnbnn_lf1_total_unc,\n",
    "    mfdnnbnn_lf1_aleatoric,\n",
    ") = model.predict(x=test_samples)\n",
    "# get lf predictions\n",
    "mfdnnbnn_lf1_y = model.predict_lf(x=test_samples, output_format=\"numpy\")\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), mfdnnbnn_lf1_hy, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (mfdnnbnn_lf1_hy - 2 * mfdnnbnn_lf1_total_unc).reshape(-1),\n",
    "    (mfdnnbnn_lf1_hy + 2 * mfdnnbnn_lf1_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, responses_lf1[\"lf\"], label=\"LF samples\")\n",
    "ax.plot(test_samples.numpy(), mfdnnbnn_lf1_y,\n",
    "        label=\"LF prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfpml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
