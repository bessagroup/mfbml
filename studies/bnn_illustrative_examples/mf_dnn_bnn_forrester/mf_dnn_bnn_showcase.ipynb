{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the configuration of the low-fidelity model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# get the accuracy metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from mfbml.methods.mf_dnn_bnn import MFDNNBNN\n",
    "from mfbml.methods.bnn import BNNWrapper\n",
    "from mfbml.methods.sequential_mf_bnn import SequentialMFBNN\n",
    "from mfbml.problems.high_dimension_problems import MengCase1\n",
    "from mfbml.metrics import (\n",
    "    mean_log_likelihood_value,\n",
    "    normalized_mae,\n",
    "    normalized_rmse,\n",
    ")\n",
    "# fix the random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function\n",
    "func = MengCase1(noise_std=0.0)\n",
    "# generate samples (21 HF samples, 201 LF samples)\n",
    "lf_samples = torch.linspace(0, 1, 201).reshape(-1, 1)\n",
    "hf_samples = lf_samples[::10]  # sample every 5 points\n",
    "# hf_samples = torch.linspace(0.0, 1.0, 20).reshape(-1, 1)\n",
    "\n",
    "# generate responses\n",
    "lf1_responses = func.lf1(lf_samples, noise_lf=0.05)\n",
    "lf2_responses = func.lf2(lf_samples, noise_lf=0.05)\n",
    "lf3_responses = func.lf3(lf_samples, noise_lf=0.05)\n",
    "# get the high-fidelity responses\n",
    "hf_responses = func.hf(hf_samples, noise_hf=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configure\n",
    "samples = {\"lf\": lf_samples, \"hf\": hf_samples}\n",
    "\n",
    "# dataset of lf1 and hf\n",
    "responses_lf1 = {\"lf\": lf1_responses,\n",
    "                 \"hf\": hf_responses}\n",
    "# dataset of lf2 and hf\n",
    "responses_lf2 = {\"lf\": lf2_responses,\n",
    "                 \"hf\": hf_responses}\n",
    "# dataset of lf2 and hf\n",
    "responses_lf3 = {\"lf\": lf3_responses,\n",
    "                 \"hf\": hf_responses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the test points\n",
    "test_samples = torch.linspace(0, 1, 1001).reshape(-1, 1)\n",
    "# noiseless responses\n",
    "test_hf_responses_noiseless = func.hf(test_samples, noise_hf=0.0)\n",
    "test_lf1_responses_noiseless = func.lf1(test_samples, noise_lf=0.0)\n",
    "test_lf2_responses_noiseless = func.lf2(test_samples, noise_lf=0.0)\n",
    "test_lf3_responses_noiseless = func.lf3(test_samples, noise_lf=0.0)\n",
    "# noise responses\n",
    "test_hf_responses = func.hf(test_samples, noise_hf=0.05)\n",
    "\n",
    "# plot the function\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_samples, test_hf_responses_noiseless, label=\"HF\")\n",
    "ax.plot(test_samples, test_lf1_responses_noiseless, label=\"LF1\")\n",
    "ax.plot(test_samples, test_lf2_responses_noiseless, label=\"LF2\")\n",
    "ax.plot(test_samples, test_lf3_responses_noiseless, label=\"LF3\")\n",
    "\n",
    "# plot the samples\n",
    "ax.scatter(hf_samples, hf_responses, label=\"HF samples\")\n",
    "ax.scatter(lf_samples, lf1_responses, label=\"LF1 samples\")\n",
    "ax.scatter(lf_samples, lf2_responses, label=\"LF2 samples\")\n",
    "ax.scatter(lf_samples, lf3_responses, label=\"LF3 samples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between HF and LF samples\n",
    "print(\"Correlation between HF and LF samples\")\n",
    "print(pearsonr(test_hf_responses_noiseless.flatten(),\n",
    "      test_lf1_responses_noiseless.flatten()))\n",
    "print(pearsonr(test_hf_responses_noiseless.flatten(),\n",
    "      test_lf2_responses_noiseless.flatten()))\n",
    "print(pearsonr(test_hf_responses_noiseless.flatten(),\n",
    "      test_lf3_responses_noiseless.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations for DNN and BNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the configuration of the low-fidelity model\n",
    "lf_configure = {\n",
    "    \"in_features\": 1,\n",
    "    \"hidden_features\": [50, 50],\n",
    "    \"out_features\": 1,\n",
    "    \"activation\": \"Tanh\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.000001,\n",
    "    \"loss\": \"mse\",\n",
    "}\n",
    "# create the configuration of the high-fidelity model\n",
    "hf_parallel_configure = {\n",
    "    \"in_features\": 1,\n",
    "    \"hidden_features\": [50, 50],\n",
    "    \"out_features\": 1,\n",
    "    \"activation\": \"Tanh\",\n",
    "    \"lr\": 0.001,\n",
    "    \"sigma\": 0.05,\n",
    "}\n",
    "#\n",
    "hf_sequential_configure = {\n",
    "    \"in_features\": 2,\n",
    "    \"hidden_features\": [50, 50],\n",
    "    \"out_features\": 1,\n",
    "    \"activation\": \"Tanh\",\n",
    "    \"lr\": 0.001,\n",
    "    \"sigma\": 0.05,\n",
    "}\n",
    "\n",
    "# lf train config\n",
    "lf_train_config = {\n",
    "    \"batch_size\": None,\n",
    "    \"num_epochs\": 10000,\n",
    "    \"print_iter\": 100,\n",
    "    \"data_split\": False,\n",
    "}\n",
    "hf_train_config = {\n",
    "    \"num_epochs\": 20000,\n",
    "    \"sample_freq\": 100,\n",
    "    \"print_info\": True,\n",
    "    \"burn_in_epochs\": 10000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train standard BNN using HF data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the responses\n",
    "hf_responses_mean = hf_responses.mean().numpy()\n",
    "hf_responses_std = hf_responses.std().numpy()\n",
    "hf_responses_scaled = (hf_responses.clone() -\n",
    "                       hf_responses_mean) / hf_responses_std\n",
    "\n",
    "# create the sf_bnn model\n",
    "sigma_scale = float(0.05 / hf_responses_std)\n",
    "bnn_model = BNNWrapper(\n",
    "    in_features=1,\n",
    "    hidden_features=[50, 50],\n",
    "    out_features=1,\n",
    "    activation=\"Tanh\",\n",
    "    lr=0.001,\n",
    "    sigma=sigma_scale,\n",
    ")\n",
    "# train the model\n",
    "bnn_model.train(\n",
    "    x=hf_samples,\n",
    "    y=hf_responses_scaled,\n",
    "    num_epochs=20000,\n",
    "    sample_freq=100,\n",
    "    burn_in_epochs=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the posterior of bnn\n",
    "(bnn_y,\n",
    " bnn_epistemic,\n",
    " bnn_total_unc,\n",
    " bnn_aleatoric) = bnn_model.predict(\n",
    "    x=test_samples)\n",
    "# scale the prediction\n",
    "bnn_y = bnn_y * hf_responses_std + hf_responses_mean\n",
    "bnn_total_unc = bnn_total_unc * hf_responses_std\n",
    "bnn_epistemic = bnn_epistemic * hf_responses_std\n",
    "bnn_aleatoric = bnn_aleatoric * hf_responses_std\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "plt.plot(test_samples.numpy(), bnn_y, label=\"hf prediction\")\n",
    "plt.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (bnn_y - 2 * bnn_total_unc).reshape(-1),\n",
    "    (bnn_y + 2 * bnn_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training of sequential MF-DNN-BNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential mf-bnn\n",
    "smf_bnn_lf1 = SequentialMFBNN(\n",
    "    design_space=torch.Tensor([[0, 1]]),\n",
    "    lf_configure=lf_configure,\n",
    "    hf_configure=hf_sequential_configure,\n",
    ")\n",
    "\n",
    "smf_bnn_lf1.train(\n",
    "    samples=samples,\n",
    "    responses=responses_lf1,\n",
    "    lf_train_config=lf_train_config,\n",
    "    hf_train_config=hf_train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the posterior of sequential mf-bnn\n",
    "(\n",
    "    smf_bnn_lf1_hy,\n",
    "    smf_bnn_lf1_epistemic,\n",
    "    smf_bnn_lf1_total_unc,\n",
    "    smf_bnn_lf1_aleatoric,\n",
    ") = smf_bnn_lf1.predict(x=test_samples)\n",
    "# get lf predictions\n",
    "smf_bnn_lf1_y = smf_bnn_lf1.predict_lf(x=test_samples, output_format=\"numpy\")\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), smf_bnn_lf1_hy, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (smf_bnn_lf1_hy - 2 * smf_bnn_lf1_total_unc).reshape(-1),\n",
    "    (smf_bnn_lf1_hy + 2 * smf_bnn_lf1_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, lf1_responses, label=\"LF samples\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    smf_bnn_lf1_y,\n",
    "    label=\"LF prediction\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the MFDNNBNN object\n",
    "mfdnnbnn_lf1 = MFDNNBNN(\n",
    "    design_space=torch.Tensor([[0, 1]]),\n",
    "    lf_configure=lf_configure,\n",
    "    hf_configure=hf_parallel_configure,\n",
    "    beta_optimize=True,\n",
    "    lf_order=1,\n",
    "    beta_bounds=[-5, 5],\n",
    "    discrepancy_normalization=\"hf\",\n",
    ")\n",
    "# define beta\n",
    "mfdnnbnn_lf1.train(\n",
    "    samples=samples,\n",
    "    responses=responses_lf1,\n",
    "    lf_train_config=lf_train_config,\n",
    "    hf_train_config=hf_train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the posterior of mf-dnn-bnn\n",
    "(\n",
    "    mfdnnbnn_lf1_hy,\n",
    "    mfdnnbnn_lf1_epistemic,\n",
    "    mfdnnbnn_lf1_total_unc,\n",
    "    mfdnnbnn_lf1_aleatoric,\n",
    ") = mfdnnbnn_lf1.predict(x=test_samples)\n",
    "# get lf predictions\n",
    "mfdnnbnn_lf1_y = mfdnnbnn_lf1.predict_lf(x=test_samples, output_format=\"numpy\")\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), mfdnnbnn_lf1_hy, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (mfdnnbnn_lf1_hy - 2 * mfdnnbnn_lf1_total_unc).reshape(-1),\n",
    "    (mfdnnbnn_lf1_hy + 2 * mfdnnbnn_lf1_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, lf1_responses, label=\"LF samples\")\n",
    "ax.plot(test_samples.numpy(), mfdnnbnn_lf1_y,\n",
    "        label=\"LF prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mf-dnn-bnn with different orders\n",
    "mfdnnbnn_lf1_order_2 = MFDNNBNN(\n",
    "    design_space=torch.Tensor([[0, 1]]),\n",
    "    lf_configure=lf_configure,\n",
    "    hf_configure={\n",
    "        \"in_features\": 1,\n",
    "        \"hidden_features\": [50, 50],\n",
    "        \"out_features\": 1,\n",
    "        \"activation\": \"Tanh\",\n",
    "        \"lr\": 0.01,\n",
    "        \"sigma\": 0.05,\n",
    "    },\n",
    "    beta_optimize=True,\n",
    "    lf_order=2,\n",
    "    beta_bounds=[-5, 5],\n",
    "    discrepancy_normalization=\"hf\",\n",
    ")\n",
    "\n",
    "# train the model\n",
    "mfdnnbnn_lf1_order_2.train(\n",
    "    samples=samples,\n",
    "    responses=responses_lf1,\n",
    "    lf_train_config=lf_train_config,\n",
    "    hf_train_config={\n",
    "        \"num_epochs\": 30000,\n",
    "        \"sample_freq\": 100,\n",
    "        \"print_info\": True,\n",
    "        \"burn_in_epochs\": 20000,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the posterior of mf-dnn-bnn\n",
    "(\n",
    "    mfdnnbnn_lf1_order_2_hy,\n",
    "    mfdnnbnn_lf1_order_2_epistemic,\n",
    "    mfdnnbnn_lf1_order_2_total_unc,\n",
    "    mfdnnbnn_lf1_order_2_aleatoric,\n",
    ") = mfdnnbnn_lf1_order_2.predict(x=test_samples)\n",
    "# get lf predictions\n",
    "mfdnnbnn_lf1_order_2_y = mfdnnbnn_lf1_order_2.predict_lf(\n",
    "    x=test_samples, output_format=\"numpy\")\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), mfdnnbnn_lf1_order_2_hy, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (mfdnnbnn_lf1_order_2_hy - 2 * mfdnnbnn_lf1_order_2_total_unc).reshape(-1),\n",
    "    (mfdnnbnn_lf1_order_2_hy + 2 * mfdnnbnn_lf1_order_2_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, lf1_responses, label=\"LF samples\")\n",
    "ax.plot(test_samples.numpy(),\n",
    "        mfdnnbnn_lf1_order_2_y,\n",
    "        label=\"LF prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfdnnbnn_lf1_order_2.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mf models for the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential mf-bnn\n",
    "smf_bnn_lf2 = SequentialMFBNN(\n",
    "    design_space=torch.Tensor([[0, 1]]),\n",
    "    lf_configure=lf_configure,\n",
    "    hf_configure=hf_sequential_configure,\n",
    ")\n",
    "\n",
    "smf_bnn_lf2.train(\n",
    "    samples=samples,\n",
    "    responses=responses_lf2,\n",
    "    lf_train_config=lf_train_config,\n",
    "    hf_train_config=hf_train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the posterior of sequential mf-bnn\n",
    "(\n",
    "    smf_bnn_lf2_hy,\n",
    "    smf_bnn_lf2_epistemic,\n",
    "    smf_bnn_lf2_total_unc,\n",
    "    smf_bnn_lf2_aleatoric,\n",
    ") = smf_bnn_lf2.predict(x=test_samples)\n",
    "# get lf predictions\n",
    "smf_bnn_lf2_y = smf_bnn_lf2.predict_lf(x=test_samples, output_format=\"numpy\")\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), smf_bnn_lf2_hy, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (smf_bnn_lf2_hy - 2 * smf_bnn_lf1_total_unc).reshape(-1),\n",
    "    (smf_bnn_lf2_hy + 2 * smf_bnn_lf1_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, lf2_responses, label=\"LF samples\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    smf_bnn_lf2_y,\n",
    "    label=\"LF prediction\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the MFDNNBNN object\n",
    "mfdnnbnn_lf2 = MFDNNBNN(\n",
    "    design_space=torch.Tensor([[0, 1]]),\n",
    "    lf_configure=lf_configure,\n",
    "    hf_configure=hf_parallel_configure,\n",
    "    beta_optimize=True,\n",
    "    lf_order=1,\n",
    "    beta_bounds=[-5, 5],\n",
    "    discrepancy_normalization=\"hf\",\n",
    ")\n",
    "# define beta\n",
    "mfdnnbnn_lf2.train(\n",
    "    samples=samples,\n",
    "    responses=responses_lf2,\n",
    "    lf_train_config=lf_train_config,\n",
    "    hf_train_config=hf_train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    hy_proposed_lf2,\n",
    "    epistemic_proposed_lf2,\n",
    "    total_unc_proposed_lf2,\n",
    "    aleatoric_proposed_lf2,\n",
    ") = mfdnnbnn_lf2.predict(x=test_samples)\n",
    "# lf prediction\n",
    "lf2_y_proposed = mfdnnbnn_lf2.predict_lf(test_samples)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), hy_proposed_lf2, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (hy_proposed_lf2 - 2 * total_unc_proposed_lf2).reshape(-1),\n",
    "    (hy_proposed_lf2 + 2 * total_unc_proposed_lf2).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, lf2_responses, label=\"LF samples\")\n",
    "ax.plot(test_samples.numpy(), lf2_y_proposed.detach().numpy(),\n",
    "        label=\"LF prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for datset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential mf-bnn\n",
    "smf_bnn_lf3 = SequentialMFBNN(\n",
    "    design_space=torch.Tensor([[0, 1]]),\n",
    "    lf_configure=lf_configure,\n",
    "    hf_configure=hf_sequential_configure,\n",
    ")\n",
    "\n",
    "smf_bnn_lf3.train(\n",
    "    samples=samples,\n",
    "    responses=responses_lf3,\n",
    "    lf_train_config = {\n",
    "    \"batch_size\": None,\n",
    "    \"num_epochs\": 30000,\n",
    "    \"print_iter\": 100,\n",
    "    \"data_split\": False,\n",
    "},\n",
    "    hf_train_config=hf_train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "(\n",
    "    smf_bnn_lf3_hy,\n",
    "    smf_bnn_lf3_epistemic,\n",
    "    smf_bnn_lf3_total_unc,\n",
    "    smf_bnn_lf3_aleatoric,\n",
    ") = smf_bnn_lf3.predict(x=test_samples)\n",
    "# get lf predictions\n",
    "smf_bnn_lf3_y = smf_bnn_lf3.predict_lf(x=test_samples, output_format=\"numpy\")\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), smf_bnn_lf3_hy, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (smf_bnn_lf3_hy - 2 * smf_bnn_lf3_total_unc).reshape(-1),\n",
    "    (smf_bnn_lf3_hy + 2 * smf_bnn_lf3_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, lf3_responses, label=\"LF samples\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    smf_bnn_lf3_y,\n",
    "    label=\"LF prediction\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the MFDNNBNN object\n",
    "mfdnnbnn_lf3 = MFDNNBNN(\n",
    "    design_space=torch.Tensor([[0, 1]]),\n",
    "    lf_configure=lf_configure,\n",
    "    hf_configure=hf_parallel_configure,\n",
    "    beta_optimize=True,\n",
    "    lf_order=1,\n",
    "    beta_bounds=[-5, 5],\n",
    "    discrepancy_normalization=\"hf\",\n",
    ")\n",
    "# define beta\n",
    "mfdnnbnn_lf3.train(\n",
    "    samples=samples,\n",
    "    responses=responses_lf3,\n",
    "    lf_train_config = {\n",
    "    \"batch_size\": None,\n",
    "    \"num_epochs\": 30000,\n",
    "    \"print_iter\": 100,\n",
    "    \"data_split\": False,\n",
    "},\n",
    "    hf_train_config=hf_train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "(\n",
    "    hy_proposed_lf3,\n",
    "    epistemic_proposed_lf3,\n",
    "    total_unc_proposed_lf3,\n",
    "    aleatoric_proposed_lf3,\n",
    ") = mfdnnbnn_lf3.predict(x=test_samples)\n",
    "# lf prediction\n",
    "lf3_y_proposed = mfdnnbnn_lf3.predict_lf(test_samples)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(hf_samples, hf_responses, \"o\", label=\"hf\")\n",
    "ax.plot(test_samples.numpy(), hy_proposed_lf3, label=\"hf prediction\")\n",
    "ax.plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    label=\"hf ground truth\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (hy_proposed_lf3 - 2 * total_unc_proposed_lf3).reshape(-1),\n",
    "    (hy_proposed_lf3 + 2 * total_unc_proposed_lf3).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    label=\"uncertainty\",\n",
    ")\n",
    "# plot lf samples\n",
    "ax.scatter(lf_samples, lf3_responses, label=\"LF samples\")\n",
    "ax.plot(test_samples.numpy(), lf3_y_proposed.detach().numpy(),\n",
    "        label=\"LF prediction\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the beta values\n",
    "print(\"Beta values\")\n",
    "print(mfdnnbnn_lf1.beta)\n",
    "print(mfdnnbnn_lf1_order_2.beta)\n",
    "print(mfdnnbnn_lf2.beta)\n",
    "print(mfdnnbnn_lf3.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the models on the accuracy metrics\n",
    "# bnn model\n",
    "# calculate the mse\n",
    "import pandas as pd\n",
    "bnn_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), bnn_y)\n",
    "bnn_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), bnn_y)\n",
    "bnn_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), bnn_y, bnn_total_unc)\n",
    "bnn_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), bnn_y)\n",
    "# print the results\n",
    "print(\"BNN model\")\n",
    "print(f\"Normalized RMSE: {bnn_mse}\")\n",
    "print(f\"Normalized MAE: {bnn_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {bnn_nll}\")\n",
    "print(f\"R2: {bnn_r2}\")\n",
    "\n",
    "# sequential mf-bnn on lf1\n",
    "# calculate the mse\n",
    "smf_bnn_lf1_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf1_hy)\n",
    "smf_bnn_lf1_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf1_hy)\n",
    "smf_bnn_lf1_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf1_hy, smf_bnn_lf1_total_unc)\n",
    "smf_bnn_lf1_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf1_hy)\n",
    "# print the results\n",
    "print(\"Sequential MF-BNN on LF1\")\n",
    "print(f\"Normalized RMSE: {smf_bnn_lf1_mse}\")\n",
    "print(f\"Normalized MAE: {smf_bnn_lf1_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {smf_bnn_lf1_nll}\")\n",
    "print(f\"R2: {smf_bnn_lf1_r2}\")\n",
    "\n",
    "# mf-dnn-bnn on lf1\n",
    "# calculate the mse\n",
    "mfdnnbnn_lf1_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_hy)\n",
    "mfdnnbnn_lf1_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_hy)\n",
    "mfdnnbnn_lf1_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_hy, mfdnnbnn_lf1_total_unc)\n",
    "mfdnnbnn_lf1_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_hy)\n",
    "# print the results\n",
    "print(\"MF-DNN-BNN on LF1\")\n",
    "print(f\"Normalized RMSE: {mfdnnbnn_lf1_mse}\")\n",
    "print(f\"Normalized MAE: {mfdnnbnn_lf1_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {mfdnnbnn_lf1_nll}\")\n",
    "\n",
    "# mf-dnn-bnn on lf1 with order 2\n",
    "# calculate the mse\n",
    "mfdnnbnn_lf1_order_2_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_order_2_hy)\n",
    "mfdnnbnn_lf1_order_2_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_order_2_hy)\n",
    "mfdnnbnn_lf1_order_2_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_order_2_hy, mfdnnbnn_lf1_order_2_total_unc)\n",
    "mfdnnbnn_lf1_order_2_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), mfdnnbnn_lf1_order_2_hy)\n",
    "# print the results\n",
    "print(\"MF-DNN-BNN on LF1 with order 2\")\n",
    "print(f\"Normalized RMSE: {mfdnnbnn_lf1_order_2_mse}\")\n",
    "print(f\"Normalized MAE: {mfdnnbnn_lf1_order_2_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {mfdnnbnn_lf1_order_2_nll}\")\n",
    "print(f\"R2: {mfdnnbnn_lf1_order_2_r2}\")\n",
    "\n",
    "\n",
    "# sequential mf-bnn on lf2\n",
    "# calculate the mse\n",
    "smf_bnn_lf2_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf2_hy)\n",
    "smf_bnn_lf2_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf2_hy)\n",
    "smf_bnn_lf2_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf2_hy, smf_bnn_lf2_total_unc)\n",
    "smf_bnn_lf2_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf2_hy)\n",
    "# print the results\n",
    "print(\"Sequential MF-BNN on LF2\")\n",
    "print(f\"Normalized RMSE: {smf_bnn_lf2_mse}\")\n",
    "print(f\"Normalized MAE: {smf_bnn_lf2_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {smf_bnn_lf2_nll}\")\n",
    "print(f\"R2: {smf_bnn_lf2_r2}\")\n",
    "\n",
    "# mf-dnn-bnn on lf2\n",
    "# calculate the mse\n",
    "mfdnnbnn_lf2_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf2)\n",
    "mfdnnbnn_lf2_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf2)\n",
    "mfdnnbnn_lf2_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf2, total_unc_proposed_lf2)\n",
    "mfdnnbnn_lf2_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf2)\n",
    "# print the results\n",
    "print(\"MF-DNN-BNN on LF2\")\n",
    "print(f\"Normalized RMSE: {mfdnnbnn_lf2_mse}\")\n",
    "print(f\"Normalized MAE: {mfdnnbnn_lf2_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {mfdnnbnn_lf2_nll}\")\n",
    "print(f\"R2: {mfdnnbnn_lf2_r2}\")\n",
    "\n",
    "# sequential mf-bnn on lf3\n",
    "# calculate the mse\n",
    "smf_bnn_lf3_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf3_hy)\n",
    "smf_bnn_lf3_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf3_hy)\n",
    "smf_bnn_lf3_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf3_hy, smf_bnn_lf3_total_unc)\n",
    "smf_bnn_lf3_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), smf_bnn_lf3_hy)\n",
    "\n",
    "# print the results\n",
    "print(\"Sequential MF-BNN on LF3\")\n",
    "print(f\"Normalized RMSE: {smf_bnn_lf3_mse}\")\n",
    "print(f\"Normalized MAE: {smf_bnn_lf3_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {smf_bnn_lf3_nll}\")\n",
    "print(f\"R2: {smf_bnn_lf3_r2}\")\n",
    "\n",
    "# mf-dnn-bnn on lf3\n",
    "# calculate the mse\n",
    "mfdnnbnn_lf3_mse = normalized_rmse(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf3)\n",
    "mfdnnbnn_lf3_nmae = normalized_mae(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf3)\n",
    "mfdnnbnn_lf3_nll = mean_log_likelihood_value(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf3, total_unc_proposed_lf3)\n",
    "mfdnnbnn_lf3_r2 = r2_score(\n",
    "    test_hf_responses_noiseless.numpy(), hy_proposed_lf3)\n",
    "# print the results\n",
    "print(\"MF-DNN-BNN on LF3\")\n",
    "print(f\"Normalized RMSE: {mfdnnbnn_lf3_mse}\")\n",
    "print(f\"Normalized MAE: {mfdnnbnn_lf3_nmae}\")\n",
    "print(f\"Normalized Log Likelihood: {mfdnnbnn_lf3_nll}\")\n",
    "print(f\"R2: {mfdnnbnn_lf3_r2}\")\n",
    "\n",
    "# save the results in pandas dataframe\n",
    "results = {\n",
    "    \"Model\": [\"BNN\", \"Sequential MF-BNN LF1\", \"MF-DNN-BNN LF1\",\n",
    "              \"MF-DNN-BNN LF1 Order 2\", \"Sequential MF-BNN LF2\",\n",
    "              \"MF-DNN-BNN LF2\", \"Sequential MF-BNN LF3\", \"MF-DNN-BNN LF3\"],\n",
    "    \"Normalized RMSE\": [bnn_mse, smf_bnn_lf1_mse, mfdnnbnn_lf1_mse,\n",
    "                        mfdnnbnn_lf1_order_2_mse, smf_bnn_lf2_mse,\n",
    "                        mfdnnbnn_lf2_mse, smf_bnn_lf3_mse, mfdnnbnn_lf3_mse],\n",
    "    \"Normalized MAE\": [bnn_nmae, smf_bnn_lf1_nmae, mfdnnbnn_lf1_nmae,\n",
    "                       mfdnnbnn_lf1_order_2_nmae, smf_bnn_lf2_nmae,\n",
    "                       mfdnnbnn_lf2_nmae, smf_bnn_lf3_nmae, mfdnnbnn_lf3_nmae],\n",
    "    \"Normalized Log Likelihood\": [bnn_nll, smf_bnn_lf1_nll, mfdnnbnn_lf1_nll,\n",
    "                                  mfdnnbnn_lf1_order_2_nll, smf_bnn_lf2_nll,\n",
    "                                  mfdnnbnn_lf2_nll, smf_bnn_lf3_nll, mfdnnbnn_lf3_nll],\n",
    "    \"R2\": [bnn_r2, smf_bnn_lf1_r2, mfdnnbnn_lf1_r2,\n",
    "           mfdnnbnn_lf1_order_2_r2, smf_bnn_lf2_r2,\n",
    "           mfdnnbnn_lf2_r2, smf_bnn_lf3_r2, mfdnnbnn_lf3_r2],\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "# save the results\n",
    "results_df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction of the models into one figure where the first row is the\n",
    "# prediction of bnn (with one subfigure), and the second row is the prediction of sequential mf-bnn mf-dnn-bnn on lf1 (with two subfigures)\n",
    "# the third row is the prediction of sequential mf-bnn mf-dnn-bnn on lf2 (with two subfigures)\n",
    "# the forth row is the prediction of sequential mf-bnn mf-dnn-bnn on lf3  (with two subfigures)\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 12))\n",
    "# plot the prediction of bnn\n",
    "axs[0, 0].plot(hf_samples, hf_responses, \"kx\",\n",
    "                linewidth=2,\n",
    "                markersize=10,label=\"HF samples\")\n",
    "axs[0, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "     \"--\",\n",
    "    color=\"#33BBEE\",\n",
    "    linewidth=2,\n",
    "    label=\"HF truth\",\n",
    ")\n",
    "\n",
    "axs[0, 0].plot(test_samples.numpy(),\n",
    "                bnn_y, \n",
    "               \"-\",\n",
    "    color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\",)\n",
    "\n",
    "axs[0, 0].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (bnn_y - 2 * bnn_total_unc).reshape(-1),\n",
    "    (bnn_y + 2 * bnn_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    color=\"#BBBBBB\",\n",
    "    label=\"CI interval\",\n",
    ")\n",
    "# axs[0, 0].legend()\n",
    "axs[0, 0].set_title(\"BNN\")\n",
    "\n",
    "# plot the prediction of mf-dnn-bnn on lf1 with order1\n",
    "axs[0, 1].plot(hf_samples, hf_responses, \"kx\", linewidth=2,\n",
    "    markersize=10,\n",
    "    label=\"HF samples\",)\n",
    "axs[0, 1].plot(lf_samples, lf1_responses,\n",
    "                  \"+\",color=\"#AA4499\", linewidth=2,\n",
    "markersize=10,\n",
    "    alpha=0.5,\n",
    "    label=\"LF samples\",)\n",
    "axs[0, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    \"--\",\n",
    "    color=\"#33BBEE\",\n",
    "    linewidth=2,\n",
    "    label=\"HF truth\",\n",
    ")\n",
    "# plot the lf truth\n",
    "axs[0, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_lf1_responses_noiseless.numpy(),\n",
    "    \"--\", color=\"#EE7733\",\n",
    "    linewidth=2,\n",
    "    label=\"LF truth\",\n",
    ")\n",
    "\n",
    "axs[0, 1].plot(test_samples.numpy(), \n",
    "               mfdnnbnn_lf1_hy,  \"-\",\n",
    "    color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\",)\n",
    "axs[0, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    mfdnnbnn_lf1_order_2_y,\n",
    "    \"-\", color=\"#0077BB\",\n",
    "           linewidth=2,\n",
    "    label=\"LF prediction\",\n",
    ")\n",
    "axs[0, 1].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (mfdnnbnn_lf1_hy - 2 * mfdnnbnn_lf1_total_unc).reshape(-1),\n",
    "    (mfdnnbnn_lf1_hy + 2 * mfdnnbnn_lf1_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    color=\"#BBBBBB\",\n",
    "    label=\"CI interval\",\n",
    ")\n",
    "\n",
    "# axs[0, 1].legend()\n",
    "axs[0, 1].set_title(\"DNN-LR-BNN (LF data 1)\")\n",
    "\n",
    "\n",
    "# plot the prediction of sequential mf-bnn on lf1\n",
    "axs[1, 0].plot(hf_samples, hf_responses, \n",
    "               \"kx\", linewidth=2,\n",
    "    markersize=10,\n",
    "    label=\"HF samples\",)\n",
    "axs[1, 0].plot(lf_samples, lf1_responses,  '+',\n",
    "               color=\"#AA4499\",\n",
    "               linewidth=2,\n",
    "                  markersize=10,\n",
    "    alpha=0.5,\n",
    "    label=\"LF samples\",)\n",
    "axs[1, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    \"--\",\n",
    "    color=\"#33BBEE\",\n",
    "    linewidth=2,\n",
    "    label=\"HF truth\",\n",
    ")\n",
    "axs[1, 0].plot(test_samples.numpy(), smf_bnn_lf1_hy, \n",
    "               \"-\", color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\",)\n",
    "# plot the lf truth\n",
    "axs[1, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_lf1_responses_noiseless.numpy(),\n",
    "    \"--\", color=\"#EE7733\",\n",
    "    linewidth=2,\n",
    "    label=\"LF truth\",\n",
    ")\n",
    "# \n",
    "axs[1, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    smf_bnn_lf1_y,\n",
    "    \"-\",\n",
    "    color=\"#0077BB\",\n",
    "    linewidth=2,\n",
    "    \n",
    "    label=\"LF prediction\"\n",
    ")\n",
    "axs[1, 0].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (smf_bnn_lf1_hy - 2 * smf_bnn_lf1_total_unc).reshape(-1),\n",
    "    (smf_bnn_lf1_hy + 2 * smf_bnn_lf1_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    color=\"#BBBBBB\",\n",
    "    label=\"CI interval\",\n",
    ")\n",
    "\n",
    "# axs[1, 0].legend()\n",
    "axs[1, 0].set_title(\"DNN-BNN (LF data 1)\")\n",
    "\n",
    "# plot the prediction of mf-dnn-bnn on lf1\n",
    "axs[1, 1].plot(hf_samples, hf_responses, \"kx\",\n",
    "               linewidth=2,\n",
    "    markersize=10,\n",
    "    label=\"HF samples\")\n",
    "# with lf order 2\n",
    "axs[1, 1].plot(test_samples.numpy(), mfdnnbnn_lf1_order_2_hy,\n",
    "                \"-\", color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\")\n",
    "axs[1, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    \"--\",\n",
    "    color=\"#33BBEE\",\n",
    "    linewidth=2,\n",
    "    label=\"HF truth\",\n",
    ")\n",
    "# with lf order 2\n",
    "axs[1, 1].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (mfdnnbnn_lf1_order_2_hy - 2 * mfdnnbnn_lf1_order_2_total_unc).reshape(-1),\n",
    "    (mfdnnbnn_lf1_order_2_hy + 2 * mfdnnbnn_lf1_order_2_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    color=\"#BBBBBB\",\n",
    "    label=\"CI interval\",\n",
    ")\n",
    "axs[1, 1].plot(lf_samples, lf1_responses, \n",
    "                  '+', color=\"#AA4499\", linewidth=2,\n",
    "    markersize=10,\n",
    "    alpha=0.5,\n",
    "    label=\"LF samples\")\n",
    "axs[1, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    mfdnnbnn_lf1_y,\n",
    "    \"-\",\n",
    "    color=\"#0077BB\",\n",
    "    linewidth=2,\n",
    "    label=\"LF prediction\"\n",
    ")\n",
    "# axs[1, 1].legend()\n",
    "axs[1, 1].set_title(\"DNN-LR-BNN (LF data 1 with order 2)\")\n",
    "# plot the prediction of sequential mf-bnn on lf2\n",
    "axs[2, 0].plot(hf_samples, hf_responses, \n",
    "                \"kx\", linewidth=2,\n",
    "     markersize=10,\n",
    "     label=\"HF samples\",)\n",
    "axs[2, 0].plot(test_samples.numpy(), smf_bnn_lf2_hy, \n",
    "               \"-\", color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\",)\n",
    "axs[2, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    \"--\",\n",
    "    color=\"#33BBEE\",\n",
    "    linewidth=2,\n",
    "    label=\"HF truth\",\n",
    ")\n",
    "axs[2, 0].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (smf_bnn_lf2_hy - 2 * smf_bnn_lf2_total_unc).reshape(-1),\n",
    "    (smf_bnn_lf2_hy + 2 * smf_bnn_lf2_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    color=\"#BBBBBB\",\n",
    "    label=\"CI interval\",\n",
    "\n",
    ")\n",
    "axs[2, 0].plot(lf_samples, lf2_responses,\n",
    "                  '+', color=\"#AA4499\", linewidth=2,\n",
    "    markersize=10,\n",
    "    alpha=0.5,\n",
    "    label=\"LF samples\")\n",
    "\n",
    "axs[2, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    smf_bnn_lf2_y,\n",
    "    \"-\",\n",
    "    color=\"#0077BB\",\n",
    "    linewidth=2,\n",
    ")\n",
    "# axs[2, 0].legend()\n",
    "axs[2, 0].set_title(\"DNN-BNN (LF data 2)\")\n",
    "\n",
    "# plot the prediction of mf-dnn-bnn on lf2\n",
    "axs[2, 1].plot(hf_samples, hf_responses, \n",
    "                \"kx\", linewidth=2,\n",
    "    markersize=10,\n",
    "    label=\"HF samples\")\n",
    "axs[2, 1].plot(test_samples.numpy(), hy_proposed_lf2, \n",
    "               \"-\", color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\")\n",
    "axs[2, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    \"--\",\n",
    "    color=\"#33BBEE\",\n",
    ")\n",
    "axs[2, 1].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (hy_proposed_lf2 - 2 * total_unc_proposed_lf2).reshape(-1),\n",
    "    (hy_proposed_lf2 + 2 * total_unc_proposed_lf2).reshape(-1),\n",
    "    alpha=0.5,\n",
    "     color=\"#BBBBBB\",\n",
    "    label=\"CI interval\"\n",
    ")\n",
    "axs[2, 1].plot(lf_samples, lf2_responses, \n",
    "                  '+', color=\"#AA4499\", linewidth=2,\n",
    "    markersize=10,\n",
    "    alpha=0.5,\n",
    "    label=\"LF samples\")\n",
    "\n",
    "axs[2, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    lf2_y_proposed.detach().numpy(),\n",
    "    \"-\",\n",
    "    color=\"#0077BB\",\n",
    "    linewidth=2,\n",
    "    label=\"LF prediction\"\n",
    ")\n",
    "# axs[2, 1].legend()\n",
    "axs[2, 1].set_title(\"DNN-LR-BNN (LF data 2)\")\n",
    "\n",
    "# plot the prediction of sequential mf-bnn on lf3\n",
    "axs[3, 0].plot(hf_samples, hf_responses,\n",
    "               'kx', linewidth=2,\n",
    "               markersize=10,\n",
    "               label=\"HF samples\")\n",
    "axs[3, 0].plot(test_samples.numpy(), smf_bnn_lf3_hy, \n",
    "               \"-\", color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\")\n",
    "axs[3, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    \"--\",\n",
    "    color=\"#33BBEE\",\n",
    "    linewidth=2,\n",
    "    label=\"HF truth\",\n",
    ")\n",
    "axs[3, 0].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (smf_bnn_lf3_hy - 2 * smf_bnn_lf3_total_unc).reshape(-1),\n",
    "    (smf_bnn_lf3_hy + 2 * smf_bnn_lf3_total_unc).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    color=\"#BBBBBB\",\n",
    "    label=\"CI interval\",\n",
    ")\n",
    "axs[3, 0].plot(lf_samples, lf3_responses,   \n",
    "                  '+', color=\"#AA4499\", linewidth=2,\n",
    "    markersize=10,\n",
    "    alpha=0.5,\n",
    "    label=\"LF samples\")\n",
    "axs[3, 0].plot(\n",
    "    test_samples.numpy(),\n",
    "    smf_bnn_lf3_y,\n",
    "    \"-\",\n",
    "    color=\"#0077BB\",\n",
    "    linewidth=2,\n",
    "    label=\"LF prediction\"\n",
    "\n",
    ")\n",
    "# axs[3, 0].legend()\n",
    "axs[3, 0].set_title(\"DNN-BNN (LF data 3)\")\n",
    "\n",
    "# plot the prediction of mf-dnn-bnn on lf3\n",
    "axs[3, 1].plot(hf_samples, hf_responses, \n",
    "               \"kx\", linewidth=2,\n",
    "    markersize=10,\n",
    "    label=\"HF samples\")\n",
    "axs[3, 1].plot(test_samples.numpy(), hy_proposed_lf3, \n",
    "               \"-\", color=\"#CC3311\",\n",
    "    linewidth=2,\n",
    "    label=\"HF prediction\")\n",
    "axs[3, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    test_hf_responses_noiseless.numpy(),\n",
    "    \"--\",\n",
    "    color=\"#33BBEE\",\n",
    "    linewidth=2,\n",
    ")\n",
    "axs[3, 1].fill_between(\n",
    "    test_samples.flatten().numpy(),\n",
    "    (hy_proposed_lf3 - 2 * total_unc_proposed_lf3).reshape(-1),\n",
    "    (hy_proposed_lf3 + 2 * total_unc_proposed_lf3).reshape(-1),\n",
    "    alpha=0.5,\n",
    "    color=\"#BBBBBB\",\n",
    "    label=\"CI interval\",\n",
    "\n",
    ")\n",
    "axs[3, 1].plot(lf_samples, lf3_responses, \n",
    "                  '+', color=\"#AA4499\", linewidth=2,\n",
    "    markersize=10,\n",
    "    alpha=0.5,\n",
    "    label=\"LF samples\")\n",
    "axs[3, 1].plot(\n",
    "    test_samples.numpy(),\n",
    "    lf3_y_proposed.detach().numpy(),\n",
    "    \"-\",\n",
    "    color=\"#0077BB\",\n",
    "    linewidth=2,\n",
    ")\n",
    "# axs[3, 1].legend()\n",
    "axs[3, 1].set_title(\"DNN-LR-BNN (LF data 3)\")\n",
    "# set the axis labels x for the bottom row and y for the left column\n",
    "for ii in range(4):\n",
    "    for jj in range(2):\n",
    "        if ii == 3:\n",
    "            axs[ii, jj].set_xlabel(\"x\", fontsize=12)\n",
    "        if jj == 0:\n",
    "            axs[ii, jj].set_ylabel(\"y\", fontsize=12)\n",
    "# set the line width of the axes\n",
    "for ii in range(4):\n",
    "    for jj in range(2):\n",
    "        axs[ii, jj].spines[\"top\"].set_linewidth(1.5)\n",
    "        axs[ii, jj].spines[\"right\"].set_linewidth(1.5)\n",
    "        axs[ii, jj].spines[\"left\"].set_linewidth(1.5)\n",
    "        axs[ii, jj].spines[\"bottom\"].set_linewidth(1.5)\n",
    "        # font size of the axis\n",
    "        axs[ii, jj].tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "        # set the y limit to be [-1.5, 1.5  ]\n",
    "        # axs[ii, jj].set_ylim([-2, 1.5])\n",
    "# set legende for the first subfigure  and put it outside the figure\n",
    "axs[0, 1].legend(loc='upper left', bbox_to_anchor=(-1.2, 1.3), fontsize=10, \n",
    "                 frameon=False, ncol=47)\n",
    "        \n",
    "# space of subfigures \n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfpml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
